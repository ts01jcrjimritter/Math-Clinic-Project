{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spire_Notebook\n",
    "\n",
    "This notebook reads forecast data from NOAA and observation data from Spire, correlates entries, by date and location, and feeds it to an SKlearn machine learning module, with the goal of improving on the wind velocity forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read_Grib\n",
    "\n",
    "Input files\n",
    "This section reads Grib files from NOAA with weather forecast elements.   The grib files are laid out in multi-dimensional arrrays by latitude, longitude, and elevation.  The grib files are created four times a day, at midnight,  6:00 am, 12:00 noon, and 6:00 pm.  NOAA creates grib files with several forecast lead times, the Spire team is using files created at 6:00 am with a 6 hour lead time. This module uses a start date and end date variable to to determine the date range that it will read in. This module also also uses a latitude and longitude range to limit the forecast data to grid locations in Colorado. \n",
    "\n",
    "Output files\n",
    "A numpy array is created from the Read_Grib section with rows that have latitude, longitude,  wind speeds, temperature, dew point, surface pressure, relative humidity, and the date for each.  The latitude and longitude values in the forecast files serve as the grid for the multi-dimensional array. The Read_Grib notebook reads all grib files for a date range, and aggregates that data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate    #install tabulate for reporting\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Nio\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import datetime\n",
    "import   pandas as  pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import wget  \n",
    "from datetime import timedelta, date\n",
    "import scipy.stats as st\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2018       # use Nio to read data from the grb2 files from NOAA.\n",
    "end_year = 2019              \n",
    "start_day = 1101       # set the start_date and end_date to choose a range of dates to readfrom\n",
    "end_day = 129\n",
    "timevalue = '12:00:00'\n",
    "hhmm = '0600'\n",
    "fcs = '006'\n",
    "\n",
    "def daterange(start_date, end_date):      #set up date range for the loop to read forecast data\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "start_date = date(start_year, (int((start_day - (start_day % 100)) / 100)), (start_day % 100))\n",
    "end_date = date(end_year, (int((end_day - (end_day % 100)) / 100)), (end_day % 100)) + timedelta(days=1)\n",
    "\n",
    "fdata = []\n",
    "lat = []\n",
    "lon = []\n",
    "date1 = [[[]]]\n",
    "ele = [1, 2, 3]\n",
    "la = 0\n",
    "lat_init = 90.0\n",
    "while la < 361:\n",
    "    lat.append(lat_init)\n",
    "    la += 1\n",
    "    lat_init -= .5\n",
    "    \n",
    "lo = 0\n",
    "lon_init = 0.0\n",
    "while lo < 720:\n",
    "    lon.append(lon_init)\n",
    "    lo += 1\n",
    "    lon_init += .5\n",
    "\n",
    "bottom = 107           #  Set limits for Latitudes\n",
    "top = 98              #  and Longitudes in \n",
    "right = 502            #  in Colorado  to limit the \n",
    "left = 518             #  volume of data\n",
    "fdata=[[ele[0],lat[0],lon[0],99.0,99.0,99.0,99.0,start_date]]\n",
    "#for date in range(start_date,end_date):\n",
    "for pdate in daterange(start_date, end_date):\n",
    "    idate = pdate.strftime(\"%m%d\")\n",
    "    iyear = pdate.strftime(\"%Y\")\n",
    "    tt = \"\"\n",
    "    \n",
    "    year = iyear\n",
    "    \n",
    "    dt1 = [str(int(year)),'-',str(int((int(idate) - (int(idate) % 100)) / 100)),'-',\n",
    "        str((int(idate)) % 100),' ',timevalue]\n",
    "    dt2 = tt.join(dt1)\n",
    "    datetime2 = dt2\n",
    "    date_time_str = datetime2\n",
    "    date_time_obj = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')\n",
    "    t = \"\"\n",
    "    a = ['/home/jovyan/spire/Input_Data/gfs_4_',str(year),str(idate),'_',str(hhmm),'_',\n",
    "    str(fcs),'.grb2']\n",
    "    file1 = t.join(a) \n",
    "    file_name = file1\n",
    "   \n",
    "    header_file = Nio.open_file(file_name, \"r\", format=\"grib\")            #use Nio.open\n",
    "    fdata3 = header_file.variables['UGRD_P0_L103_GLL0'][0:3,0:,0:]        # read u component of \n",
    "    fdata4 = header_file.variables['VGRD_P0_L103_GLL0'][0:3,0:,0:]\n",
    "    fdata5 = header_file.variables['TMP_P0_L1_GLL0'][0:,0:]\n",
    "    fdata6 = header_file.variables['DPT_P0_L103_GLL0'][0:,0:]\n",
    "    fdata7 = header_file.variables['RH_P0_L4_GLL0'][0:,0:]\n",
    "    fdata8 = header_file.variables['PRMSL_P0_L101_GLL0'][0:,0:]\n",
    "    fdata9 = header_file.variables['PRES_P0_L1_GLL0'][0:,0:]\n",
    "    fdata10 = header_file.variables['HGT_P0_L100_GLL0'][0:,0:]\n",
    "    \n",
    "    la = top\n",
    "    el = 0\n",
    "    \n",
    "    \n",
    "    while la < bottom:\n",
    "        lo = right\n",
    "         \n",
    "        while lo < left:\n",
    "          \n",
    "            el = 0\n",
    "            while el < 1:\n",
    "                \n",
    "                fdata2=[ele[el],lat[la],lon[lo],fdata3[el,la,lo], fdata4[el,la,lo],\n",
    "                fdata5[la,lo],fdata6[la,lo],fdata7[la,lo],fdata8[la,lo],\n",
    "                fdata9[la,lo],\n",
    "                #fdata10[la,lo],\n",
    "                date_time_obj]\n",
    "                 \n",
    "                fdata.append(fdata2)          #save the forecast data elements to an array\n",
    "                el +=1\n",
    "        \n",
    "            lo += 1\n",
    "        la += 1\n",
    "del fdata[0]\n",
    "len(fdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"El\",\"Lat\",\"Lon\",\"U WSp\",\"V WSp\",\"Temp\",\"Dew Pt\",\"R Hum\",\"RPMSL\",\"PRES\",\"Date\"]\n",
    "print(tabulate(fdata[0:10],headers=headers))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read_Excel\n",
    "\n",
    "Inputs This section reads in Observation files from Spire, they are observed weather measurements taken at 58 weather stations across Colorado. The observations are taken at various times a day, but all times are not consistent for all weather stations, the best time for consistency is 12:00 noon, so those observations will be used.\n",
    "\n",
    "Outputs A numpy array is created from the Read_Excel section with rows that have latitude, longitude, wind speeds, temperature, pressure, visibility and cloud ceiling measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set start_day to the first date \n",
    "#Set end_day to the day after the last date\n",
    "#Initialize array for mydata \n",
    "#Read the files with observation data.\n",
    "mydata = [['Stat',0,0,99,1200,22,33,44,55]]\n",
    "\n",
    "start_year = 2018   # use Pandas to read data from the .xlsx files from Spire\n",
    "end_year = 2019              \n",
    "start_day = 1101    # set the start_day, start_year  and end_day, end_year to choose a range of dates to read from\n",
    "end_day = 129\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "start_date = date(start_year, (int((start_day - (start_day % 100)) / 100)), (start_day % 100))\n",
    "end_date = date(end_year, (int((end_day - (end_day % 100)) / 100)), (end_day % 100)) + timedelta(days=1)\n",
    "\n",
    "for pdate in daterange(start_date,end_date):\n",
    "    idate = pdate.strftime(\"%m%d\")\n",
    "    iyear = pdate.strftime(\"%Y\")\n",
    "    year = iyear\n",
    "    \n",
    "    \n",
    "    t = \"\"\n",
    "    a = ['/home/jovyan/spire/Input_Data/obs',str(year),str(idate),'.xlsx']\n",
    "    file1 = t.join(a) \n",
    "    file = file1\n",
    "    # \n",
    "    xl = pd.ExcelFile(file) # Print the sheet names\n",
    "    #print(xl.sheet_names) # Load a sheet into a DataFrame by name: df1\n",
    "    df1 = xl.parse('Sheet1')\n",
    "    df = pd.read_excel(file, sheet_name='Sheet1')\n",
    "\n",
    "\n",
    "    #print(\"Column headings:\")\n",
    "    StationID = df['Station ID']\n",
    "    Latitude = df['Latitude']\n",
    "    Longitude = df['Longitude']\n",
    "    Elevation = df['Elevation']\n",
    "    Time = df['Time']\n",
    "    Temperature = df['Temperature']\n",
    "    U = df['U']\n",
    "    V = df['V']\n",
    "    WindSpeed = df['Wind Speed']\n",
    "    Visibility = df['Visibility']\n",
    "    Ceiling = df['Ceiling']\n",
    "    #print(df.columns)\n",
    "    #for i in range(10):\n",
    "    #   print '%-12i%-12i' % (10 ** i, 20 ** i)\n",
    "    #print(\"Station ID   Latitude  Longitude  Elevation Time                 Temperature   U   V   Wind Speed \")\n",
    "\n",
    "    i = 0\n",
    "    #while loop to build array\n",
    "    while i < len(df):\n",
    "        mydata2=[StationID[i],Latitude[i],Longitude[i],Elevation[i],Time[i],Temperature[i],U[i],V[i],WindSpeed[i]]\n",
    "        mydata.append(mydata2)\n",
    "        i+=1     \n",
    "del mydata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid_Corr\n",
    "\n",
    "The Grid_Corr section reads in numpy arrays from Read_Grib and Read_Excel and correlates grid points from the forecast data to weather station grid points. It is set up to correlate those points using 3 different methods, one assigns the station grid point to the closest forecast grid point. The second method calculates the four forecast grid points that surround each station grid point, and the third similar, but assigns wind speed values using an average of the 4 surrounding points. Later in the process, we intend to evaluate these three methods to determine which is optimal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"St ID\",\"Lat\",\"Lon\",\"Elev\", \"Time\",\"Temp\",\"U\",\"V\",\"Wind Speed\"]\n",
    "print(tabulate(mydata[0:10],headers=headers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array(mydata)\n",
    "x5 = np.array(x2[0:,1:10])\n",
    "x5a = np.array(x2[0:,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Lat\",\"Lon\",\"Elev\", \"Time\",\"Temp\",\"U\",\"V\",\"Wind Speed\"]\n",
    "print(tabulate(x5[0:10],headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a file with lat, lon, grid lat, grid lon, and all observed variables\n",
    "j = 0\n",
    "i = 0\n",
    "m = len(x5)\n",
    "n = 10\n",
    "x6 = np.array(x5[0:,0:])\n",
    "#\n",
    "for i in range(len(x5)):\n",
    "    x6[i,0] = x5[i,0]\n",
    "    x6[i,1] = x5[i,1]\n",
    "    x6[i,4] = x5[i,2]\n",
    "    #x6[i,5] = x5[i,3]\n",
    "    #x6[i,5] = 00.0\n",
    "    x6[i,6] = x5[i,4]\n",
    "    x6[i,7] = x5[i,5]\n",
    "    #x6[i,8] = x5[i,6]\n",
    "    #x6[i,9] = x5[i,7]\n",
    "    a=x6[i,0]\n",
    "    c = a - (a % 1)\n",
    "   \n",
    "    if x6[i,0] - c < .25:         #  calculate the closest grid latitude and longitudes to each station location\n",
    "        x6[i,2] = c\n",
    "    elif x6[i,0] - c < .5:\n",
    "        x6[i,2] = a + (.5 -(a % 1))\n",
    "    elif x6[i,0] - c < .75:\n",
    "        x6[i,2] = a + (.5 -(a % 1))\n",
    "    else:\n",
    "        x6[i,2] = a + (1 - (a % 1))\n",
    "    \n",
    "    b=x6[i,1]\n",
    "    d = b - (b % 1)\n",
    "  \n",
    "    if x6[i,1] - d < .25:\n",
    "        x6[i,3] = d\n",
    "    elif x6[i,1] - d < .5:\n",
    "        x6[i,3] = b + (.5 -(b % 1))\n",
    "    elif x6[i,1] - d < .75:\n",
    "        x6[i,3] = b + (.5 -(b % 1))\n",
    "    else:\n",
    "        x6[i,3] = b + (1 - (b %1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0                                            # this step inserts the time stamp into the array \n",
    "x7 = [[0.0,0.0,0.0,0.0,99,1200,22,33,44,55]]   # initialize x7 with dummy data\n",
    "while i < len(x6):\n",
    "    xa=[x6[i,0],x6[i,1],x6[i,2],x6[i,3],x6[i,4],x5[i,3],x6[i,6],x6[i,7],x5[i,6],x5[i,7]]\n",
    "    x7.append(xa)\n",
    "    i+=1     \n",
    "del x7[0]\n",
    "x7a = np.array(x7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x8 = [[0.0,0.0,0.0,0.0,99,1200,22,33,44,55]]   # initialize x8 with dummy data\n",
    "x8a = [0.0,0.0,0.0,0.0,99,1200,22,33,44,55]    # initialize x8a with dummy data\n",
    "j=0\n",
    "i=0\n",
    "for i in range(len(x7)):                       # remove records with filler data\n",
    "    if x7a[i,9] == 1e+11:\n",
    "        ii = 2\n",
    "    elif x7a[i,5].hour != 12:                  # remove records with times other than 12:00\n",
    "        ii = 2\n",
    "    else:\n",
    "        x8a = x7[i]\n",
    "        x8.append(x8a)\n",
    "        j +=1\n",
    "del x8[0]\n",
    "x8b = np.array(x8)\n",
    "x8c = np.array(x8b[:,0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=[\"Lat\",\"Lon\",\"Grid_Lat\",\"Grid_Lon\",\"Elev\",\"Time\",\"Temp\",\"U\",\"V\",\"Wind Sp\"]\n",
    "print(tabulate(x8b[0:10],headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a file with lat, lon, 4 surroundong Grid lat points, and all observed variables\n",
    "j = 0\n",
    "i = 0\n",
    "m = len(x5)\n",
    "n = 10\n",
    "x10 = np.array(x5[0:,0:])\n",
    "#x6 = [[0 for x in range(m)] for y in range(n)]\n",
    "\n",
    "for i in range(len(x5)):\n",
    "    x10[i,0] = x5[i,0]\n",
    "    x10[i,1] = x5[i,1]\n",
    "    #\n",
    "    \n",
    "    #\n",
    "    a=x10[i,0]            # Calculate the grid latitudes above and below each station latitude\n",
    "    c = a - (a % 1)\n",
    "   \n",
    "    if x10[i,0] - c < .5:   # find the latitude above and below\n",
    "        x10[i,2] = c\n",
    "        x10[i,4] = c + .5\n",
    "    else:\n",
    "        x10[i,2] = c + .5\n",
    "        x10[i,4] = c + 1\n",
    " \n",
    "    \n",
    "    b=x10[i,1]          # Calculate the grid longitudes to the right and left of each station longitude\n",
    "    d = b - (b % 1)\n",
    "  \n",
    "    if x10[i,1] - d < .5:    # find the longitude to the right and left\n",
    "        x10[i,3] = d\n",
    "        x10[i,5] = d + .5\n",
    "    else:\n",
    "        x10[i,3] = d + .5\n",
    "        x10[i,5] = d + 1\n",
    "        \n",
    "    if x10[i,0] - x10[i,2] < .25:   # find the closest grid lat\n",
    "        x10[i,6] = x10[i,2]\n",
    "        \n",
    "    elif x10[i,0] - x10[i,2] < .5:\n",
    "        x10[i,6] = x10[i,2] + .5\n",
    "        \n",
    "    elif x10[i,0] - x10[i,2] < .75:\n",
    "        x10[i,6] = x10[i,2]\n",
    "        \n",
    "    else:\n",
    "        x10[i,6] = x10[i,2] + 5\n",
    "        \n",
    "    if x10[i,1] - x10[i,3] < .25:  # find the closest grid lon\n",
    "        x10[i,7] = x10[i,3]\n",
    "        \n",
    "    elif x10[i,1] - x10[i,3] < .5:\n",
    "        x10[i,7] = x10[i,3] + .5\n",
    "        \n",
    "    elif x10[i,1] - x10[i,3] < .75:\n",
    "        x10[i,7] = x10[i,3] \n",
    "        \n",
    "    else:\n",
    "        x10[i,7] = x10[i,5] + .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "x11 = [[0.0,0.0,0.0,0.0,0.0, 0.0,0.0,0.0,99,1200,22,33,44,55]]  #initialize x11 with dummy variables\n",
    "while i < len(x6):\n",
    "    xa=[x10[i,0],x10[i,1],x10[i,2],x10[i,3],x10[i,4],x10[i,5],x10[i,6],x10[i,7],x5[i,2],x5[i,3],x5[i,4],x5[i,5],x5[i,6],x5[i,7]]\n",
    "    x11.append(xa)\n",
    "    i+=1     \n",
    "del x11[0]\n",
    "x11a = np.array(x11)\n",
    "len(x11a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=[\"Lat\",\"Lon\",\"Gla1\",\"Glo1\",\"Gla2\",\"Glo2\",\"CLa1\",\"CLo1\",\"Elev\",\"Time\",\"Temp\",\"U\",\"V\",\"Wind Sp\"]\n",
    "print(tabulate(x11[0:10],headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step does data cleanup\n",
    "# it removes records with invalid wind speed and \n",
    "# removes records with timestamps not = 12:00\n",
    "\n",
    "x12 = [[0.0,0.0,0.0,0.0,0.0,0.0,0.0,99,1200,22,33,44,55]]   # initialize x12 with dummy data\n",
    "x12a = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,99,1200,22,33,44,55]    # initialize x12a with dummy data\n",
    "j=0\n",
    "i=0\n",
    "for i in range(len(x11)):                                   # take out records with filler data\n",
    "    if x11a[i,13] == 1e+11:\n",
    "        ii = 2\n",
    "    elif x11a[i,9].hour != 12:                              #  take out records with timestamps not equal to 12:00\n",
    "        ii = 2\n",
    "    else:\n",
    "        x12a = x11a[i]\n",
    "        x12.append(x12a)\n",
    "        j +=1\n",
    "del x12[0]\n",
    "len(x12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x13 = sorted(x12, key=itemgetter(0,1))  #Sort by lat and lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=[\"Lat\",\"Lon\",\"Gla1\",\"Glo1\",\"Gla2\",\"Glo2\",\"CLa1\",\"CLo1\",\"Elev\",\"Time\",\"Temp\",\"U\",\"V\",\"Wind Sp\"]\n",
    "print(tabulate(x13[0:10],headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x13a = np.array(x13)\n",
    "x14 = [[0.0,0.0,0.0,0.0,0.0,0.0,0.0,99,1200,22,33,44,55]]     # initialize x14 with dummy data\n",
    "x14a = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,99,1200,22,33,44,55]      # initialize x14a with dummy data\n",
    "j= len(x13) - 1\n",
    "i=0\n",
    "for i in range(j):\n",
    "    if (x13a[i,0] == x13a[i+1,0] and x13a[i,1] == x13a[i+1,1]):   # remove records with duplicate lat and lon\n",
    "        i += 1\n",
    "    else:\n",
    "        x14a = x13[i]\n",
    "        x14.append(x14a)\n",
    "        i +=1\n",
    "del x14[0]\n",
    "x15a = np.array(x14)\n",
    "x15 = np.array(x15a[:,:8])\n",
    "len(x15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x15g = sorted(x15, key=itemgetter(6,7))  #Sort by  closest lat lon\n",
    "x15h = np.array(x15g)\n",
    "len(x15h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x15c = [[0.0,0.0,0.0,0.0,0.0,0.0,0.0,99,12]]         # initialize x15c with dummy data\n",
    "x15d = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,99,12]           # initialize x15d with dummy data\n",
    "j= len(x15h) - 1\n",
    "i=0\n",
    "for i in range(j):\n",
    "    if (x15h[i,6] == x15h[i+1,6] and x15h[i,7] == x15h[i+1,7]):  # remove records with duplicate lat and lon\n",
    "        i += 1\n",
    "    else:\n",
    "        x15d = x15h[i]\n",
    "        x15c.append(x15d)\n",
    "        i +=1\n",
    "del x15c[0]\n",
    "x15e = np.array(x15c)\n",
    "x15f = np.array(x15e)\n",
    "len(x15f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=[\"Lat\",\"Lon\",\"Gla1\",\"Glo1\",\"Gla2\",\"Glo2\",\"CLa1\",\"CLo1\",\"Elev\",\"Time\",\"Temp\",\"U\",\"V\",\"Wind Sp\"]\n",
    "print(tabulate(x15f[0:10],headers=headers))   #one record for each Station Location,  sorted by lat and lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract_Data\n",
    "\n",
    "The Extract_Data section matches data values from the forecast files to the observation files and prepares files to be input to the Sklearn module. The Sklearn module requires that the forecast and observation files are identical in size and that entries are sorted so that they correlate to eachother. The Extract_Data notebook uses the merge function from Pandas to match up records from each file by grid location and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"El\",\"Lat\",\"Lon\",\"U WSp\",\"V WSp\",\"Temp\",\"Dew Pt\",\"R Hum\",\"RPMSL\",\"PRES\",\"Date\"]\n",
    "print(tabulate(fdata[0:10],headers=headers)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata2 = sorted(fdata, key=itemgetter(10,1,2))  #Sort by date, lat, and then lo\n",
    "fdata2a = np.array(fdata2)\n",
    "len(fdata2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata3 = np.insert(fdata2a,5,   #   convert the u and v components of wind speed to \n",
    "                                #   wind speed\n",
    "                   \n",
    "((((fdata2a[:,3])**2) + ((fdata2a[:,4])**2))**.5),axis=1)\n",
    "fdata3a=np.array(fdata3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"El\",\"Lat\",\"Lon\",\"U WSp\",\"V WSp\",\"Wind SP\",\"Temp\",\"Dew Pt\",\"RH\",\"RPMSL\",\"PRES\",\"Date\"]\n",
    "print(tabulate(fdata3a[0:10],headers=headers))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Lat\",\"Lon\",\"GLat1\",\"GLon1\",\"GLat2\",\"GLon2\",\"CLat\",\"CLon\",\"Elev\",\"Time\",\n",
    "\"Temp\", \"U\", \"V\", \"WindSp\" ]\n",
    "print(tabulate(x13[0:10],headers=headers))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell calculates the forecast wind speed to correlate to each weather station observed wind  speed. This step creates an array that will be used for subsequent analysis steps. The layout of the array is listed below.  The code calculates the forecast wind speed 3 different ways:\n",
    "\n",
    "  1     Wind speed from the forecast grid point closest to the weather station location.\n",
    "  \n",
    "  2     Wind speed averaged from the 4 forecast grid points that surround the weather station location.\n",
    "  \n",
    "  3     Wind speed from a weighted average from the 4 forecast grid points that surround the weather \n",
    "      station location.  \n",
    "      \n",
    "  **** Warning  ******  this cell is long running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wtaveragecalc2(b):  #bilinear interpolation function to calculate a weighted average for 4 surrounding grid points\n",
    "    return ( (1/(np.abs((cc - aa) * (bb - dd)))) *                          # 1 / (x1 - x2)(y1 - y2)\n",
    "    (x13aa[i,b - 6] * ((np.abs((x13aa[i,0] - cc) * (x13aa[i,1] - dd)))) +   # f(x11)*(x2 - x)(y2 - y)\n",
    "    x13aa[i,b - 5] * ((np.abs((x13aa[i,0] - cc) * (x13aa[i,1] - bb)))) +    # f(x21)*(x - x1)(y2 - y)\n",
    "    x13aa[i,b - 4] * ((np.abs((x13aa[i,0] - aa) * (x13aa[i,1] - dd)))) +    # f(x11)*(x2 - x)(y - y1)\n",
    "    x13aa[i,b - 3] * ((np.abs((x13aa[i,0] - aa) * (x13aa[i,1] - bb))))))    # f(x22)*(x - x1)(y - y1)\n",
    "#Bilinear Interpolation formula from \n",
    "# https://en.wikipedia.org/wiki/Bilinear_interpolation\n",
    "def averagecalc(b):\n",
    "    return (x13aa[i,b - 5] + x13aa[i,b - 4] + x13aa[i,b - 3] + x13aa[i, b - 2]) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # add columns to x13aa for 4 surrounding grid \n",
    "   # point windspeed values and the closest \n",
    "   # grid point wind speed value\n",
    "    \n",
    "x13ee = np.array(x13)\n",
    "x13aa = np.append(x13ee,np.zeros([len(x13),28]),1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i = 0                                                                   \n",
    "for i in range(len(x13aa)):                                               \n",
    "     aa = x13aa[i,4] # upper lat                                        \n",
    "     bb = x13aa[i,3] # left lon\n",
    "     cc = x13aa[i,2] # lower lat\n",
    "     dd = x13aa[i,5] # right lon\n",
    "     ee = x13aa[i,6] # closest lat\n",
    "     ff = x13aa[i,7] # closest lon\n",
    "    \n",
    "     dt = x13aa[i,9] # date time stamp\n",
    "    \n",
    "     #  match the observed windspeeds to the four forecast grid points above, \n",
    "     #  below, and to the right and left\n",
    "    \n",
    "     temp1a = np.where((fdata3a[:,1] == aa ) & (fdata3a[:,2] == bb) & (fdata3a[:,11] == dt),fdata3a[:,5],-1)\n",
    "     itemindex1 = np.where(temp1a != -1)\n",
    "     itemindex1a = np.array(itemindex1)\n",
    "     aaa = int(itemindex1a[0,0])\n",
    "        \n",
    "     x13aa[i,14] = fdata3a[aaa,5]   # add wind speed for the upper left grid point\n",
    "     x13aa[i,21] = fdata3a[aaa,6]   # add temperature for the upper left grid point\n",
    "     x13aa[i,28] = fdata3a[aaa,9]   # add PRMSL for the upper left grid point\n",
    "     x13aa[i,35] = fdata3a[aaa,10]  # add Pressure for the upper left grid point\n",
    "    \n",
    "     temp1b = np.where((fdata3a[:,1] == aa ) & (fdata3a[:,2] == dd) & (fdata3a[:,11] == dt),fdata3a[:,5],-1)\n",
    "     itemindex2 = np.where(temp1b != -1)\n",
    "     itemindex2a = np.array(itemindex2)\n",
    "     bbb = int(itemindex2a[0,0])\n",
    "     x13aa[i,15] = fdata3a[bbb,5]   # add wind speed for the upper right grid point\n",
    "     x13aa[i,22] = fdata3a[bbb,6]   # add temperature for the upper right grid point\n",
    "     x13aa[i,29] = fdata3a[bbb,9]   # add PRMSL for the upper right grid point\n",
    "     x13aa[i,36] = fdata3a[bbb,10]  # add Pressure for the upper right grid point\n",
    "     \n",
    "     temp1c = np.where((fdata3a[:,1] == cc ) & (fdata3a[:,2] == bb) & (fdata3a[:,11] == dt),fdata3a[:,5],-1)\n",
    "     itemindex3 = np.where(temp1c != -1)\n",
    "     itemindex3a = np.array(itemindex3)\n",
    "     ccc = int(itemindex3a[0,0])\n",
    "     x13aa[i,16] = fdata3a[ccc,5]   # add wind speed for the lower left grid point\n",
    "     x13aa[i,23] = fdata3a[ccc,6]   # add temperature for the lower left grid point\n",
    "     x13aa[i,30] = fdata3a[ccc,9]   # add PRMSL for the lower left grid point\n",
    "     x13aa[i,37] = fdata3a[ccc,10]  # add Pressure for the lower left grid point\n",
    "     \n",
    "     temp1d = np.where((fdata3a[:,1] == cc ) & (fdata3a[:,2] == dd) & (fdata3a[:,11] == dt),fdata3a[:,5],-1)\n",
    "     itemindex4 = np.where(temp1d != -1)\n",
    "     itemindex4a = np.array(itemindex4)\n",
    "     ddd = int(itemindex4a[0,0])\n",
    "     x13aa[i,17] = fdata3a[ddd,5]   # add wind speed for the lower right grid point\n",
    "     x13aa[i,24] = fdata3a[ddd,6]   # add temperature for the lower right grid point\n",
    "     x13aa[i,31] = fdata3a[ddd,9]   # add PRMSL for the lower right grid point\n",
    "     x13aa[i,38] = fdata3a[ddd,10]  # add Pressure for the lower right grid point\n",
    "     \n",
    "     temp1e = np.where((fdata3a[:,1] == ee ) & (fdata3a[:,2] == ff) & (fdata3a[:,11] == dt),fdata3a[:,5],-1)\n",
    "     itemindex5 = np.where(temp1e != -1)\n",
    "     itemindex5a = np.array(itemindex5)\n",
    "     eee = int(itemindex5a[0,0])\n",
    "     x13aa[i,18] = fdata3a[eee,5]   # add wind speed for the closest grid point\n",
    "     x13aa[i,25] = fdata3a[eee,6]   # add temperature for the closest grid point\n",
    "     x13aa[i,32] = fdata3a[eee,9]   # add PRMSL for the closest grid point\n",
    "     x13aa[i,39] = fdata3a[eee,10]   # add Pressure for the closest grid point\n",
    "    \n",
    "     # Calculate the average forecasted windspeed of the four grid points surrounding each \n",
    "     # observation. \n",
    "    \n",
    "     x13aa[i,19] = averagecalc(19) # Wind SP\n",
    "     x13aa[i,26] = averagecalc(26) # Temp\n",
    "     x13aa[i,33] = averagecalc(33) # PRMSL\n",
    "     x13aa[i,40] = averagecalc(40) # Press\n",
    "    \n",
    "     # Calculate a weighted  average forecasted windspeed of the four grid points \n",
    "     # surrounding each observation.\n",
    "    \n",
    "     x13aa[i,20] = wtaveragecalc2(20)   # Wind SP\n",
    "    \n",
    "     x13aa[i,27] = wtaveragecalc2(27)   # Temp\n",
    "    \n",
    "     x13aa[i,34] = wtaveragecalc2(34)   # PRMSL\n",
    "    \n",
    "     x13aa[i,41] = wtaveragecalc2(41)   # Press"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X13aa file layout\n",
    "\n",
    "  Field and Description\n",
    "  \n",
    "•  0  Station Latitude\n",
    "\n",
    "•  1  Station Longitude\n",
    "\n",
    "•  2  Grid Latitude below the Station Latitude\n",
    "\n",
    "•  3  Grid Longitude to the left of the  Station Longitude\n",
    "\n",
    "•  4  Grid Latitude above the Station Longitude\n",
    "\n",
    "•  5  Grid Longitude to the right of the Station Longitude\n",
    "\n",
    "•  6  Closest Grid Latitude to the Station Latitude\n",
    "\n",
    "•  7  Closest Grid Longitude to the Station Longitude\n",
    "\n",
    "•  8  Elevation of the station\n",
    "\n",
    "•  9  Date//Time of the observation\n",
    "\n",
    "• 10 Temperature \n",
    "\n",
    "• 11 Observed U component of windspeed in meters per second\n",
    "\n",
    "• 12 Observed V component of windspeed in meters per second\n",
    "\n",
    "• 13 Observed windspeed for the station location in meters per second\n",
    "\n",
    "• 14 Forecasted wind speed for the upper left grid point\n",
    "\n",
    "• 15 Forecasted wind speed for the upper right grid point\n",
    "\n",
    "• 16 Forecasted wind speed for the lower left grid point\n",
    "\n",
    "• 17 Forecasted wind speed for the lower right grid point\n",
    "\n",
    "• 18 Forecasted wind speed for the closest grid point\n",
    "\n",
    "• 19 Average forecasted wind speed for the 4 surrounding grid points\n",
    "\n",
    "• 20 Weighted average forecasted wind speed for the 4 surrounding grid points\n",
    "\n",
    "• 21 Forecasted temperature for the upper left grid point\n",
    "\n",
    "• 22 Forecasted temperature for the upper right grid point\n",
    "\n",
    "• 23 Forecasted temperature for the lower left grid point\n",
    "\n",
    "• 24 Forecasted temperature for the lower right grid point\n",
    "\n",
    "• 25 Forecasted temperature for the closest grid point\n",
    "\n",
    "• 26 Average forecasted temperature for the 4 surrounding grid points\n",
    "\n",
    "• 27 Weighted average forecasted temperature for the 4 surrounding grid points\n",
    "\n",
    "• 28 Forecasted Pressure reduced to Mean Sea Level for the upper left grid point\n",
    "\n",
    "• 29 Forecasted Pressure reduced to Mean Sea Level for the upper right grid point\n",
    "\n",
    "• 30 Forecasted Pressure reduced to Mean Sea Level for the lower left grid point\n",
    "\n",
    "• 31 Forecasted Pressure reduced to Mean Sea Level for the lower right grid point\n",
    "\n",
    "• 32 Forecasted Pressure reduced to Mean Sea Level for the closest grid point\n",
    "\n",
    "• 33 Average forecasted Pressure reduced to Mean Sea Level for the 4 surrounding grid\n",
    "     points\n",
    "   \n",
    "• 34 Weighted average forecasted Pressure reduced to Mean Sea Level for the 4 surrounding \n",
    "     grid points\n",
    "• 35 Forecasted Pressure for the upper left grid point\n",
    "\n",
    "• 36 Forecasted Pressure for the upper right grid point\n",
    "\n",
    "• 37 Forecasted Pressure for the lower left grid point\n",
    "\n",
    "• 38 Forecasted Pressure for the lower right grid point\n",
    "\n",
    "• 39 Forecasted Pressure for the closest grid point\n",
    "\n",
    "• 40 Average forecasted Pressure for the 4 surrounding grid points\n",
    "\n",
    "• 41 Weighted average forecasted Pressure for the 4 surrounding grid points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Lat\",\"Lon\",\"GLat1\",\"GLon1\",\"GLat2\",\"GLon2\",\"CLat\",\"CLon\",\"Elev\",\"Time\",\"Temp\",\n",
    "           \"U\", \"V\", \"WindSp\",\"UL WS\",\"UR WS\",\"LL WS\", \"LR WS\",\"Cl WS\",\"AVG WS\",\"Wtd Avg\"]\n",
    "print(tabulate(x13aa[0:2],headers=headers))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x16 = sorted(x15f, key=itemgetter(6,7))  #Sort by  closest lat lon\n",
    "x16a = np.array(x16)\n",
    "len(x16a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Lat\",\"Lon\",\"GLat1\",\"GLon1\",\"GLat2\",\"GLon2\",\"CLat\",\"CLon\"]\n",
    "print(tabulate(x16a[0:10],headers=headers))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forext13ab = np.array(x13aa[0:,[19,26,33,40]])  #\n",
    "obsext13ab = np.array(x13aa[0:,[13]])  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forext13ac = np.array(x13aa[0:,[20,27,34,41]])  #four points\n",
    "obsext13ac = np.array(x13aa[0:,[13]])  #weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file with possible grid points\n",
    "file2 = \"/home/jovyan/spire/Input_Data/Grid_Locations.xlsx\"\n",
    "\n",
    "xl2 = pd.ExcelFile(file2) # Print the sheet names\n",
    "#print(xl.sheet_names) # Load a sheet into a DataFrame by name: df1\n",
    "df12 = xl2.parse('Sheet1')\n",
    "df2 = pd.read_excel(file2, sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latitude = df2['Latitude']\n",
    "Longitude = df2['Longitude']\n",
    "headers2 = [\"Latitude\",\"Longitude\"]\n",
    "mydata3 = [[Latitude[0],Longitude[0]]]\n",
    "i = 1\n",
    "#while loop to build array\n",
    "while i < len(df2):\n",
    "    mydata4=[Latitude[i],Longitude[i]]\n",
    "    mydata3.append(mydata4)\n",
    "    i+=1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array(mydata)\n",
    "x4 = np.array(mydata3)\n",
    "x5 = np.array(x2[0:,1:10])\n",
    "x5a = np.array(x2[0:,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "def scatterplots(x) :\n",
    "    p = x.shape[1]\n",
    "    q = 'Forecast Grid Points'\n",
    "    for r in range(p - 1) :          # row variables loop:\n",
    "        for c in range(r + 1, p) :   # column variables loop:\n",
    "            plt.subplot(p - 1, p - 1, (p - 1)*r + c)\n",
    "            plt.scatter(x[:, c], x[:, r], label=str(q), color = 'b', s = 40)\n",
    "scatterplots(x4)\n",
    "def scatterplots(x) :\n",
    "    p = x.shape[1]\n",
    "    q = 'Observation Locations'\n",
    "    for r in range(p - 1) :          # row variables loop:\n",
    "        for c in range(r + 1, p) :   # column variables loop:\n",
    "            plt.subplot(p - 1, p - 1, (p - 1)*r + c)\n",
    "            plt.scatter(x[:, c], x[:, r], label=str(q), color = 'r', s = 40)\n",
    "scatterplots(x5a)\n",
    "plt.title(\"Colorado Forecast Grid Points and Weather Stations\" ,fontsize=16)\n",
    "plt.xlabel(\"Longitude\", fontsize=12)\n",
    "plt.ylabel(\"Latitude\", fontsize=12)\n",
    "plt.legend(loc='lower right', shadow=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot weather station locations over closest Grid Points\n",
    "plt.figure(figsize=(12,8))\n",
    "x7 = np.array(x6[0:,2:4])\n",
    "def scatterplots(x) :\n",
    "    p = x.shape[1]\n",
    "    q = 'Forecast Grid Points'\n",
    "    for r in range(p - 1) :          # row variables loop:\n",
    "        for c in range(r + 1, p) :   # column variables loop:\n",
    "            plt.subplot(p - 1, p - 1, (p - 1)*r + c)\n",
    "            plt.scatter(x[:, c], x[:, r], label=str(q), color = 'b', s = 10)\n",
    "scatterplots(x4)\n",
    "def scatterplots(x) :\n",
    "    p = x.shape[1]\n",
    "    q = 'Station Locations mapped to Grid Points'\n",
    "    for r in range(p - 1) :          # row variables loop:\n",
    "        for c in range(r + 1, p) :   # column variables loop:\n",
    "            plt.subplot(p - 1, p - 1, (p - 1)*r + c)\n",
    "            plt.scatter(x[:, c], x[:, r], label=str(q),  color = 'r', s = 40)\n",
    "# suptitle('Weather Station Locations', fontsize=16)\n",
    "scatterplots(x7)\n",
    "plt.title(\"Colorado Weather Station Locations mapped over Closest Forecast Grid Points\", fontsize=16)\n",
    "plt.xlabel(\"Longitude\", fontsize=12)\n",
    "plt.ylabel(\"Latitude\", fontsize=12)\n",
    "    \n",
    "plt.legend(loc='lower right', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare Observed and Forecast Wind Speeds\n",
    "WindData = np.array(x13aa)\n",
    "OWS=WindData[:,13]\n",
    "FWSC=WindData[:,18]\n",
    "FWSAV=WindData[:,19]\n",
    "FWSWTAV=WindData[:,20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "box_plot_data=[OWS,FWSC,FWSAV,FWSWTAV]\n",
    "plt.boxplot(box_plot_data,patch_artist=True,\n",
    "labels=['Observed Wind Speed','Forecast Windspeed\\nClosest Grid Point',\n",
    "    'Forecast Windspeed\\nAvg of 4 surrounding \\nGrid Points',\n",
    "    'Forecast Windspeed\\nWeighted Avg of Four \\nSurrounding Grid Points'])\n",
    "plt.title(\"Observed Wind Speeds compared with Forecast Wind Speeds\", fontsize=16)\n",
    "plt.ylabel(\"Wind Speeds in Meters per Second\", fontsize=12)\n",
    "plt.text(2, 25, r'The gold line in the boxplot is the median')\n",
    "plt.text(2, 24, r'The bottom of the box is the First Quartile')\n",
    "plt.text(2, 23, r'The top of the box is the Third Quartile')\n",
    "plt.text(2, 22, r'The length of the box is the Inter Quartile Range (IQR)')\n",
    "plt.text(2, 21, r'The whiskers are (1.5 * IQR) above the Third Quartile and below the First Quartile')\n",
    "plt.text(2, 20, r'The observations above the whisker are outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FWSCA = np.zeros(len(FWSC))\n",
    "FWSAVA = np.zeros(len(FWSC))\n",
    "FWSWTAVA = np.zeros(len(FWSC))\n",
    "Diff1 = np.zeros(len(FWSC))\n",
    "\n",
    "for i in range(len(FWSC)):\n",
    "    FWSCA[i] = np.around(FWSC[i],decimals = 2)\n",
    "    FWSAVA[i] = np.around(FWSAV[i],decimals = 2)\n",
    "    FWSWTAVA[i] = np.around(FWSWTAV[i],decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(FWSC)):\n",
    "    Diff1[i] = np.around(OWS[i] - FWSCA[i],decimals=1)\n",
    "Diff2=Diff1[0:]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hist(Diff2,bins=60)\n",
    "plt.title(\"Observed - Forecast From Closest Points     Difference Histogram\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylim(top=1400)\n",
    "plt.text(10, 1000, r'Difference of Observed and Forecaset Winspeed')\n",
    "plt.text(10, 950, r'Windspeed in Meters per Second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(FWSC)):\n",
    "    Diff1[i] = np.around(OWS[i] - FWSAVA[i],decimals=1)\n",
    "Diff2=Diff1[0:]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hist(Diff2,bins=60)\n",
    "plt.title(\"Observed - Forecast Average from Surrounding Points Difference Histogram\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylim(top=1400)\n",
    "plt.text(10, 1000, r'Difference of Observed and Forecaset Winspeed')\n",
    "plt.text(10, 950, r'Windspeed in Meters per Second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(FWSC)):\n",
    "    Diff1[i] = np.around(OWS[i] - FWSWTAVA[i],decimals=1)\n",
    "Diff2=Diff1[0:]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hist(Diff2,bins=60)\n",
    "plt.title(\"Observed - Forecast Weighted Average from Surrounding Points Difference Histogram\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylim(top=1400)\n",
    "plt.text(10, 1000, r'Difference of Observed and Forecaset Winspeed')\n",
    "plt.text(10, 950, r'Windspeed in Meters per Second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OWSbyDate = np.array(WindData[0:,[13,9]])\n",
    "FWSCbyDate = np.array(WindData[0:,[18,9]])\n",
    "FWSAVbyDate = np.array(WindData[0:,[19,9]])\n",
    "FWSWTAVbyDate = np.array(WindData[0:,[20,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WindData2=np.array(WindData[:,[9,13,18,8]])  # Set up a line graph with observed and forecast windsppeds for comparison\n",
    "for station in range(len(x15f)):\n",
    "    OWSStatbyDate = []                         # Choose one station and graph both variables over time\n",
    "    #OWSStatbyDate = [1.1, 2018-11-1]  # initialize with dummy data\n",
    "    for ii in range(len(WindData)):\n",
    "        if WindData[ii,0] == x15f[station,0] and WindData[ii,1] == x15f[station,1]:\n",
    "            OWSStatbyDate.append(WindData2[ii,0:4])\n",
    "    OWSStatbyDate2 = np.array(OWSStatbyDate)\n",
    "    titlestring1 = \"Observed and Forecast Windspeed For One Station by Date \\n  Station Location  Latitude = \"\n",
    "    titlestring2 = str(np.around(x15f[station,0],decimals=2)) + \"   Longitude = \" + str(np.around(x15f[station,1],decimals=2))\n",
    "    titlestring3 = \" \\n Station Elevation = \" + str(OWSStatbyDate[0][3]) + \" meters\"\n",
    "    titlestring = titlestring1 + titlestring2 + titlestring3\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    plt.title(titlestring) \n",
    "    plt.xlabel(\"Date\", fontsize = 12)\n",
    "    plt.ylabel(\"Wind Speed in meters per second\", fontsize = 12)\n",
    "    #plt.text(1,WindData2[14,0], r'Difference of Observed and Forecaset Winspeed')\n",
    "    plt.xlim([datetime.date(2018, 11, 1), datetime.date(2019, 1, 29)])\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.plot(OWSStatbyDate2[:,0],OWSStatbyDate2[:,1],color='b',label='Observed Wind Speed')\n",
    "    plt.plot(OWSStatbyDate2[:,0],OWSStatbyDate2[:,2],color='r',label='Forecast Wind Speed')\n",
    "    plt.legend(loc='upper right', shadow=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WindData2=np.array(WindData[:,[9,13,18,8,0,1]])  # Set up a line graph with observed and forecast windsppeds for comparison\n",
    "\n",
    "OWSStatbyDate1 = []\n",
    "OWSStatbyDate2 = []   \n",
    "OWSStatbyDate3 = []  \n",
    "OWSStatbyDate4 = [] \n",
    "\n",
    "suspects = [[37.45,254.13],[37.95,252.1],[38.1,253.83],[38.53,253.07],\n",
    "            [38.78,251.94],[38.81,253.88],[39.65,253.08],[40.04,253.63],[40.15,254.84],[40.52,253.13]]\n",
    "\n",
    "for ii in range(len(WindData2)):\n",
    "        \n",
    "        test = np.isin(suspects, WindData2[ii,[4,5]])\n",
    "        for iii in range(len(suspects)):\n",
    "            type(test[iii])\n",
    "            if test[iii][0] == True  and test[iii][1] == True:\n",
    "                OWSStatbyDate4.append(WindData2[ii,0:4])\n",
    "                \n",
    "        if WindData2[ii,3] <= 2000:\n",
    "            OWSStatbyDate1.append(WindData2[ii,0:4])\n",
    "        elif WindData2[ii,3] <= 3000:\n",
    "            OWSStatbyDate2.append(WindData2[ii,0:4])\n",
    "        else:\n",
    "            OWSStatbyDate3.append(WindData2[ii,0:4])\n",
    "OWSStatbyDate11 = np.array(OWSStatbyDate1)\n",
    "OWSStatbyDate22 = np.array(OWSStatbyDate2)\n",
    "OWSStatbyDate33 = np.array(OWSStatbyDate3)\n",
    "OWSStatbyDate44 = np.array(OWSStatbyDate4)\n",
    "titlestring1 = \"Observed and Forecast Windspeed For All Stations  \\n Color Coded by Elevation  \"\n",
    " \n",
    "titlestring = titlestring1\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title(titlestring) \n",
    "plt.ylabel(\"Observed Wind Speed in meters per second\", fontsize = 12)\n",
    "plt.xlabel(\"Forecast Wind Speed in meters per second\", fontsize = 12)\n",
    " \n",
    "plt.xticks(rotation='vertical')\n",
    "plt.scatter(OWSStatbyDate33[:,2],OWSStatbyDate33[:,1],color='y',s=5,label='Stations above 3000 meters')\n",
    "\n",
    "plt.scatter(OWSStatbyDate22[:,2],OWSStatbyDate22[:,1],color='b',s=5,label='Stations below 3000 meters')\n",
    "plt.scatter(OWSStatbyDate11[:,2],OWSStatbyDate11[:,1],color='r',s=5,label='Stations below  2000 meters')\n",
    "plt.scatter(OWSStatbyDate44[:,2],OWSStatbyDate44[:,1],color='g',s=5,label='Suspect Stations')\n",
    "plt.plot(OWSStatbyDate11[:,1],OWSStatbyDate11[:,1],color='r',label='X = Y')\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis-Linear Regression The Analysis section summarize wind speed for observation data,weather forecast data and merged dataset, and compare which wind speed(closet or averaged)we should put into the model. The Analysis notebook use df.dataframe to generate different linear regression model and use backward selection and transformation to make R^2 as big as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weather Forecast wind speed \n",
    "#fdata = np.array(forecast_for_merged)\n",
    "data=x13aa[:,18]\n",
    "quartiles = percentile(data, [25, 50, 75])\n",
    "data_min, data_max = data.min(), data.max()\n",
    "print('Min: %.3f' % data_min)\n",
    "print('Q1: %.3f' % quartiles[0])\n",
    "print('Median: %.3f' % quartiles[1])\n",
    "print('Q3: %.3f' % quartiles[2])\n",
    "print('Max: %.3f' % data_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind speed for observation\n",
    "\n",
    "odata1 = np.array(x13aa[:,13])\n",
    "data1=odata1\n",
    "quartiles = percentile(data1, [25, 50, 75])\n",
    "data1_min, data1_max = data1.min(), data1.max()\n",
    "print('Min: %.3f' % data1_min)\n",
    "print('Q1: %.3f' % quartiles[0])\n",
    "print('Median: %.3f' % quartiles[1])\n",
    "print('Q3: %.3f' % quartiles[2])\n",
    "print('Max: %.3f' % data1_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "box_plot_data=[data,data1]\n",
    "plt.boxplot(box_plot_data,patch_artist=True,labels=['Weather Forecast\\nClosest Grid Point','Observation Wind Speed'])\n",
    "#The upper and lower line represent Q1-1.5*IQR and Q3+1.5*IQR\n",
    "#IQR=Q3-Q1 which is interquatile range\n",
    "plt.title(\"Observed Wind Speeds compared with Forecast Wind Speeds\", fontsize=16)\n",
    "plt.ylabel(\"Wind Speeds in Meters per Second\", fontsize=12)\n",
    "plt.text(1, 25, r'The gold line in the boxplot is the median')\n",
    "plt.text(1, 24, r'The bottom of the box is the First Quartile')\n",
    "plt.text(1, 23, r'The top of the box is the Third Quartile')\n",
    "plt.text(1, 22, r'The length of the box is the Inter Quartile Range (IQR)')\n",
    "plt.text(1, 21, r'The whiskers are (1.5 * IQR) above the Third Quartile and below the First Quartile')\n",
    "plt.text(1, 20, r'The observations above the whisker are outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.kdeplot(data)\n",
    "sns.kdeplot(data1)\n",
    "plt.title(\"Observed Wind Speeds compared with Forecast Wind Speeds\", fontsize=16)\n",
    "plt.xlabel(\"Wind Speeds in Meters per Second\", fontsize=12)\n",
    "plt.text(10,0.30,r'The blue line represents Weather Forecast at Closest Grid Point')\n",
    "plt.text(10,0.25,r'The Orange line represents Observation Wind Speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "plt.scatter(data, data1, label = \"Relationship\",s=5,color='r')\n",
    "plt.xlabel('Weather Forecast - axis')\n",
    "plt.ylabel('Observation - axis')\n",
    "plt.title('Comparison')\n",
    "plt.legend()\n",
    "plt.plot(data,data, linestyle='solid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.DataFrame({'Glat':x13aa[:,6],'Glon':x13aa[:,7],'Elevation':x13aa[:,8],'WSobs':x13aa[:,13],'Date':x13aa[:,9],\n",
    "        'WSu':x13aa[:,11],'WSv':x13aa[:,12],'WSfor':x13aa[:,18],'Temp':x13aa[:,25],'PRMSL':x13aa[:,32],\n",
    "        'PRES':x13aa[:,39]})\n",
    "df=DataFrame(merged_data,columns=[\"Latitude\",\"Longitude\",\"Elevation\",\"WSobs\",\"Date\",\"WSu\",\"WSv\",\"WSfor\",\"Temp\",\"PRMSL\",\"PRES\"])\n",
    "X1=df[['WSfor']]\n",
    "Y1=df['WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=df[['Elevation','WSfor','Temp','PRMSL','PRES']]\n",
    "Y1=df['WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Selection\n",
    "X1=df[['Elevation','WSfor','Temp','PRES']]\n",
    "Y1=df['WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[df['Elevation']>3000].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df.index[720:1007])\n",
    "X1=df[['Elevation','WSfor','Temp','PRES']]\n",
    "Y1=df['WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df.index[2465:2660])\n",
    "X1=df[['Elevation','WSfor','Temp','PRES']]\n",
    "Y1=df['WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df.index[4799:5072])\n",
    "X1=df[['Elevation','WSfor','Temp','PRES']]\n",
    "Y1=df['WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df.index[5306:5415])\n",
    "X1=df[['Elevation','WSfor','Temp','PRES']]\n",
    "Y1=df['WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[df['Elevation']>3000].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df.index[5906:6059])\n",
    "X1=df[['Elevation','WSfor','Temp','PRES']]\n",
    "Y1=df['WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df.index[6169:6359])\n",
    "X1=df[['Elevation','WSfor','Temp','PRES']]\n",
    "Y1=df['WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df.index[6802:7065])\n",
    "X1=df[['Elevation','WSfor','Temp','PRES']]\n",
    "Y1=df['WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8), dpi=80)\n",
    "plt.scatter(df['WSobs'], df['WSfor'], label = \"Accuracy\",s=5)\n",
    "plt.xlabel('Weather forecast-Elevation < 3000')\n",
    "plt.ylabel('Observation-')\n",
    "plt.title('Comparison')\n",
    "plt.legend() \n",
    "plt.plot(Y1,Y1+0 , linestyle='solid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8), dpi=80)\n",
    "plt.scatter(Y1, model.fittedvalues, label = \"Accuracy\",s=5)\n",
    "plt.xlabel('Observation - axis')\n",
    "plt.ylabel('Fitted Wind Speed- axis')\n",
    "plt.title('Comparison')\n",
    "plt.legend() \n",
    "plt.plot(Y1,Y1+0 , linestyle='solid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(squareroot_WSobs=df['WSobs']**(1/2))\n",
    "X1=df[['WSfor','Temp','PRES']]\n",
    "Y1=df['squareroot_WSobs']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X1, Y1)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "X2 = sm.add_constant(X1)\n",
    "model = sm.OLS(Y1.astype(float), X1.astype(float)).fit()\n",
    "predictions = model.predict(X1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "plt.scatter(Y1, model.fittedvalues, label = \"Accuracy\",s=5)\n",
    "plt.xlabel('Observation - axis')\n",
    "plt.ylabel('Fitted Wind Speed- axis')\n",
    "plt.title('Comparison')\n",
    "plt.legend() \n",
    "plt.plot(Y1,Y1+0 , linestyle='solid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(Y1,model.fittedvalues ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Steps: using sklearn model\n",
    "The Neural Netwok inputs Forecast data and Wind Speeds from observations.\n",
    "We want to make a prediction model and use cross validation to test the data. The test data set is 0.2 of the whole data. Inside the trainning data, the dataset is devided into trainning data and 0.1 validation data, which is used in early stopping to avoid overfit. \n",
    "Here is the 3 month data whose data size is 11510. \n",
    "The four variables of the data are:\n",
    "19 Average forecasted wind speed for the 4 surrounding grid points, \n",
    "26 Average forecasted temperature for the 4 surrounding grid points, \n",
    "33 Average forecasted Pressure reduced to Mean Sea Level for the 4 surrounding grid points, \n",
    "40 Average forecasted Pressure for the 4 surrounding grid points\n",
    "Estimated running time: 15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avscore=0\n",
    "for t in range(5):\n",
    "    forext13ab = np.array(x13aa[0:,[19,26,33,40]])  #\n",
    "    obsext13ab = np.array(x13aa[0:,[13]])  #\n",
    "    X= forext13ab.astype(float)\n",
    "    Y= obsext13ab.astype(float)\n",
    "    Y.shape=(len(x13aa),)\n",
    "    X.shape=(len(x13aa),4)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(5,3),verbose=True,early_stopping=True,max_iter=1500)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    avscore+=mlp.score(X_test, y_test)\n",
    "s=avscore/5\n",
    "print(\"Average test set score: %f\" % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make prediction with 1 month data. The data size is only 3706. First we inport the data.Estimated running time: 9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avscore=0\n",
    "with open('/home/jovyan/spire/Input_Data/Test_Extract_Data_obsext13ab_pickle', 'rb') as f:\n",
    "    obsext13ab = pickle.load(f)\n",
    "with open('/home/jovyan/spire/Input_Data/Test_Extract_Data_forext13ab_pickle', 'rb') as f:\n",
    "    forext13ab = pickle.load(f)\n",
    "for t in range(10):\n",
    "    X= forext13ab.astype(float)\n",
    "    Y= obsext13ab.astype(float)\n",
    "    Y.shape=(3706,)\n",
    "    X.shape=(3706,4)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(5,3),verbose=True,early_stopping=True,max_iter=1500)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    avscore+=mlp.score(X_test, y_test)\n",
    "s=avscore/10\n",
    "print(\"Average test set score: %f\" % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here. The larger data size helps improve the prediction of the model. The test score is improved from 0.187 to 0.234. we will continue explore the 3 month dataset.we test the similiar transformation as the linear regression model. square root y. And we only use one pressure data to predict. Estimated running time: 23s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avscore=0\n",
    "for t in range(10):\n",
    "    forext13ab = np.array(x13aa[0:,[19,26,40]])  #\n",
    "    obsext13ab = np.array(x13aa[0:,[13]])  #\n",
    "    X= forext13ab.astype(float)\n",
    "    Y= np.sqrt(obsext13ab.astype(float))\n",
    "    Y.shape=(len(x13aa),)\n",
    "    X.shape=(len(x13aa),3)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(5,3),verbose=True,early_stopping=True,max_iter=1500)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    avscore+=mlp.score(X_test, y_test)\n",
    "s=avscore/10\n",
    "print(\"Average test set score: %f\" % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here our prediction score is not improved. Then we take the elevation of the station into consideration.\n",
    "Estimated running time 25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avscore=0\n",
    "for t in range(10):\n",
    "    forext13ab = np.array(x13aa[0:,[8,19,26,40]])  #\n",
    "    obsext13ab = np.array(x13aa[0:,[13]])  #\n",
    "    X= forext13ab.astype(float)\n",
    "    Y= obsext13ab.astype(float)\n",
    "    Y.shape=(len(x13aa),)\n",
    "    X.shape=(len(x13aa),4)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(5,3),verbose=True,early_stopping=True,max_iter=1500)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    avscore+=mlp.score(X_test, y_test)\n",
    "s=avscore/10\n",
    "print(\"Average test set score: %f\" % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the test score improved a lot.This time we will try to split wind into two direction.\n",
    "Estimated running time: 25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avscore=0\n",
    "for t in range(10):\n",
    "    forext13ab = np.array(x13aa[0:,[8,11,12,26,40]])  #\n",
    "    obsext13ab = np.array(x13aa[0:,[13]])  #\n",
    "    X= forext13ab.astype(float)\n",
    "    Y= obsext13ab.astype(float)\n",
    "    Y.shape=(len(x13aa),)\n",
    "    X.shape=(len(x13aa),5)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(5,3),verbose=True,early_stopping=True,max_iter=1500)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    avscore+=mlp.score(X_test, y_test)\n",
    "s=avscore/10\n",
    "print(\"Average test set score: %f\" % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test score decreased. Adding the real temperature data into consideration.Estimated running time: 23s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avscore=0\n",
    "for t in range(10):\n",
    "    forext13ab = np.array(x13aa[0:,[8,19,26,40,10]])  #\n",
    "    obsext13ab = np.array(x13aa[0:,[13]])  #\n",
    "    X= forext13ab.astype(float)\n",
    "    Y= obsext13ab.astype(float)\n",
    "    Y.shape=(len(x13aa),)\n",
    "    X.shape=(len(x13aa),5)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(5,3),verbose=True,early_stopping=True,max_iter=1500)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    avscore+=mlp.score(X_test, y_test)\n",
    "s=avscore/10\n",
    "print(\"Average test set score: %f\" % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing score is improving again. However we can not observe the real data of the temperation before it happend.It is also possible that this improvement is caused by more predictors.\n",
    "Next we would add lontitude and lontitude into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avscore=0\n",
    "for t in range(10):\n",
    "    forext13ab = np.array(x13aa[0:,[6,7,19,26,40,10]])  #\n",
    "    obsext13ab = np.array(x13aa[0:,[13]])  #\n",
    "    X= forext13ab.astype(float)\n",
    "    Y= obsext13ab.astype(float)\n",
    "    Y.shape=(len(x13aa),)\n",
    "    X.shape=(len(x13aa),6)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(5,3),verbose=True,early_stopping=True,max_iter=1500)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    avscore+=mlp.score(X_test, y_test)\n",
    "s=avscore/10\n",
    "print(\"Average test set score: %f\" % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not improved by the lontitude and latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now the best neural network model is the one using:\n",
    "8 Elevation of the station\n",
    "19 Average forecasted wind speed for the 4 surrounding grid points\n",
    "26 Average forecasted temperature for the 4 surrounding grid points\n",
    "40 Average forecasted Pressure for the 4 surrounding grid points\n",
    "10 Temperature\n",
    "as predictor.\n",
    "The structure of the Neural Network is also adjusted. By changing structure to 5-10-4-1, the prediction score is improved 0.1-0,2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avscore=0\n",
    "for t in range(10):\n",
    "    forext13ab = np.array(x13aa[0:,[8,19,26,40,10]])  #\n",
    "    obsext13ab = np.array(x13aa[0:,[13]])  #\n",
    "    X= forext13ab.astype(float)\n",
    "    Y= obsext13ab.astype(float)\n",
    "    Y.shape=(len(x13aa),)\n",
    "    X.shape=(len(x13aa),5)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(5,3),verbose=True,early_stopping=True,max_iter=1500)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    avscore+=mlp.score(X_test, y_test)\n",
    "s=avscore/10\n",
    "print(\"Average test set score: %f\" % s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = mlp.predict(X_test)-y_test\n",
    "sse = sum(resid**2)\n",
    "k = 86\n",
    "n = 10500\n",
    "AIC= 2*k - 2*np.log(sse)\n",
    "BIC = n*np.log(sse/n) + k*np.log(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIC is: %f\" % AIC)\n",
    "print(\"BIC is: %f\" % BIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(y_test, mlp.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the correlation and r-square of the forecast wind speed and observation wind speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(x13aa[0:,[19]])\n",
    "X=np.round(X.astype(float),decimals=5)\n",
    "Y.shape=(len(x13aa),)\n",
    "X.shape=(len(x13aa),)\n",
    "print(np.corrcoef(X, Y))\n",
    "print(r2_score(Y,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the prediction of neural network improved the performance of original prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
